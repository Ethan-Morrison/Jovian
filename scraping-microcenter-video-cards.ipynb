{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Microcenter for Graphic Card Availability\n",
    "\n",
    "TODO:\n",
    "- Introduction about web scraping\n",
    "- Introduction to microcenter\n",
    "- Explain tools used(Python, requests, Beautiful Soup, Pandas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the outline we'll follow:\n",
    "- Setup jupyter notebook\n",
    "- I'm going to scrape https://www.microcenter.com/ for graphic card data which includes name, stock status, and card URL.\n",
    "- Use functions to take the data and create a dictionary for each url. Combine dictionaries into one dataframe\n",
    "- Execute code using predetermined urls\n",
    "- Create csv file from the dataframe\n",
    "- List references and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup jupyter notebook\n",
    "Here we will setup the jupyter notebook so the notebook can be run anywhere. We will install and import the required libraries. The jovian library is used for uploading to www.jovian.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ethan-morrison/scraping-microcenter-video-cards\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ai/ethan-morrison/scraping-microcenter-video-cards\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ethan-morrison/scraping-microcenter-video-cards'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook.\n",
    "# Not required to run the notebook. Used for saving.\n",
    "jovian.commit(project=\"scraping-microcenter-video-cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ethan-morrison/scraping-microcenter-video-cards\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ai/ethan-morrison/scraping-microcenter-video-cards\n",
      "[jovian] Submitting assignment..\n",
      "[jovian] Verify your submission at https://jovian.ai/learn/zero-to-data-analyst-bootcamp/assignment/project-1-web-scraping-with-python\n"
     ]
    }
   ],
   "source": [
    "jovian.submit(assignment=\"zerotoanalyst-project1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Micro Center\n",
    "Micro Center is an electronics retailer here in the United States. They have many physical stores where you can purchase computer parts and electronic without waiting around for items to ship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a dataframe. This notebook is designed to pull multiple pages of video cards. Each page is appended to the cvs file. The dataframe is created outside the other code as to not be overwritten with each url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF created with the columns we will later fill.\n",
    "cards_df = pd.DataFrame(columns = ['Video_Card', 'In_Stock_Status', 'Card_Url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will defined a function using the libraries requests and Beautiful Soup.\n",
    "\n",
    "Requests will download our web page html code and write it to a text file. Beautiful Soup is used to pull data out of the html code text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_page(page_url):\n",
    "    #download the page\n",
    "    response = requests.get(page_url)\n",
    "    #check for successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(page_url))\n",
    "    # Parse using Beautiful soup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the variable 'doc' containing html code we will pull the required data with more defined functions.\n",
    "\n",
    "First the card names and URL address. In the html code the card name is coded in the 'a' tag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(doc):\n",
    "    # Get the a tags containing video card title \n",
    "    selection_data = 'Video Cards'\n",
    "    title_tags = doc.find_all('a', {'data-category': selection_data})\n",
    "    # This method scrapes duplicate video card entry so we remove duplicate\n",
    "    title_tags = [v for i, v in enumerate(title_tags) if i % 2 == 1]\n",
    "    video_cards = []\n",
    "    for tag in title_tags:\n",
    "        video_cards.append(tag.text)\n",
    "    return video_cards \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we will pull url information. The url is found in the 'href' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(doc):\n",
    "    selection_data = 'Video Cards'\n",
    "    title_tags = doc.find_all('a', {'data-category': selection_data})\n",
    "    # This method scrapes duplicate video card entry so we remove duplicate\n",
    "    title_tags = [v for i, v in enumerate(title_tags) if i % 2 == 1]\n",
    "    # Get URL and add to end of base url\n",
    "    base_urls = 'https//www.microcenter.com'\n",
    "    video_card_urls = []\n",
    "    for tag in title_tags:\n",
    "        video_card_urls.append(base_urls + tag['href'])\n",
    "    return video_card_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third we pull the stock status. This code brings the store location as well as the in stock amount if listed on the website. The stock status was found in the 'div' tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock(doc): \n",
    "    # Get stock tags\n",
    "    selection_data2 = 'stock'\n",
    "    status = []\n",
    "    stock_tags = doc.find_all('div', {'class': selection_data2})\n",
    "    for tag in stock_tags:\n",
    "        status.append(tag.get_text(\"|\", strip=True))\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple URL to Dataframe\n",
    "With the relevant information in variables we will combine everything in a dataframe from dictionaries created from multiple urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_availability():\n",
    "    \n",
    "    doc = get_card_page(page_url)\n",
    "    cards_dict = {\n",
    "        'Video_Card': get_title(doc),\n",
    "        'In_Stock_Status': get_stock(doc),\n",
    "        'Card_Url': get_url(doc)\n",
    "    }\n",
    "    cards_df = pd.DataFrame(cards_dict)\n",
    "    return cards_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Code\n",
    "We will use the created get_availability to complete the scraping process.\n",
    "\n",
    "Two custom urls are used for this notebook. They were predetermined from microcenter.com. They are set to use the store located in St. Louis Park, Minnesota USA. Also they are set to display 96 items per page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 1 from microcenter\n",
    "page_url = 'https://www.microcenter.com/search/search_results.aspx?Ntk=all&sortby=match&N=4294966937&myStore=false&rpp=96'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call get_availability with df.append to add each page to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df = cards_df.append(get_availability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 2 from microcenter\n",
    "page_url = 'https://www.microcenter.com/search/search_results.aspx?Ntk=all&sortby=match&N=4294966937&myStore=false&rpp=96&page=2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df = cards_df.append(get_availability())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataframe has now been created. \n",
    "\n",
    "Uncomment and run the following code if you need to clear the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cards_df = pd.DataFrame(columns = ['Video_Card', 'In_Stock_Status', 'Card_Url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV\n",
    "Creating a CSV from a dataframe is a simple task. The file Video_Card_Availability.cvs will be created on your computer. The file contains all video cards on microcenter.com with their availability and url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df.to_csv('Video_Card_Availability.cvs', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Next Step Ideas\n",
    "This web scraping project was a guided project. The references used were \"Zero to Data Analyst\" by Jovian(https://www.jovian.ai/data-analyst-bootcamp)and the youtube video \"Let's Build a Python Web Scraping Project from Scratch | Hands-On Tutorial\" by Jovian(https://www.youtube.com/watch?v=RKsLLG-bzEY). The documentation for each python library were also referenced. \n",
    "\n",
    "Future changes to the code could include some sort of loop to run both url links with one call to the get_availability function. Another change would be to find a way to create the dataframe and append to it without running separate code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
